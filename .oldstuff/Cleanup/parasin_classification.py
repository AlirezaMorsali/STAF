# -*- coding: utf-8 -*-
"""parasin-classification.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1yCYj3d0m_j5Nk5kmE55f2X6UZpqSq9PA
"""


"""# MNIST"""

pip install -q -U tensorflow-addons

import io
import numpy as np
import matplotlib.pyplot as plt

import tensorflow as tf
import tensorflow_addons as tfa
import tensorflow_datasets as tfds
from tensorflow import keras
from tensorflow.keras import layers

# Load MNIST dataset as NumPy arrays
dataset = {}
num_validation = 10000
(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()

# Preprocess the data
x_train = x_train.reshape(-1, 784).astype('float32') / 127.5 - 1.0
x_test = x_test.reshape(-1, 784).astype('float32') / 127.5 - 1.0





class ParaSineLayer(tf.keras.layers.Layer):
    def __init__(self, in_features, units, bias=True, is_first=False, omega_0=np.pi ** 3):
        super(ParaSineLayer, self).__init__()
        self.in_features = in_features
        self.units = units
        self.is_first = is_first
        self.omega_0 = omega_0

        self.dense = tf.keras.layers.Dense(self.units,
                                           use_bias=bias,
                                           kernel_initializer=self.init_weights(),
                                           input_shape=(self.in_features,))
        
    
    def init_weights(self):
        if self.is_first:
            return tf.keras.initializers.RandomUniform(minval=-1 / np.pi ** 5,
                                                       maxval= 1 / np.pi ** 5)
        else:
            return tf.keras.initializers.RandomUniform(minval=-np.sqrt(6. / self.in_features) / self.omega_0,
                                                       maxval= np.sqrt(6. / self.in_features) / self.omega_0)
    

    def build(self, input_shape):

        self.a_1 = self.add_weight(
            name='a_1',
            shape=(self.units,),
            initializer='zeros',
            trainable=True)

        self.a0 = self.add_weight(
            name='a0',
            shape=(self.units,),
            initializer='ones',
            trainable=True)
        self.w0 = self.add_weight(
            name='w0',
            shape=(self.units,),
            initializer='ones',
            trainable=True)
        self.shift0 = self.add_weight(
            name='shift0',
            shape=(self.units,),
            initializer='zeros',
            trainable=True)

        self.a1 = self.add_weight(
            name='a1',
            shape=(self.units,),
            initializer='ones',
            trainable=True)
        self.w1 = self.add_weight(
            name='w1',
            shape=(self.units,),
            initializer='ones',
            trainable=True)
        self.shift1 = self.add_weight(
            name='shift1',
            shape=(self.units,),
            initializer='zeros',
            trainable=True)


        super(ParaSineLayer, self).build(input_shape)


    def call(self, input_tensor):
        befor_activation = self.dense(input_tensor)
        after_activation = self.a_1 * self.omega_0 * befor_activation + \
                           self.a0 * tf.sin(self.w0 * self.omega_0 * befor_activation + self.shift0) + \
                           self.a1 * tf.cos(self.w1 * self.omega_0 * befor_activation + self.shift1)
        return after_activation
    
    
    def compute_output_shape(self, input_shape):
        return (input_shape[0], self.units)





def ParaSINNetwork():
    tf.keras.backend.clear_session()

    inputs = layers.Input(shape=(784,))
    
    ## Parametric
    X = ParaSineLayer(784, 256, is_first=True)(inputs)
    
    ## Gelu
    #X = layers.Dense(1024, activation=None, use_bias=False, kernel_initializer=tf.keras.initializers.RandomUniform(minval=-1 / np.pi ** 5, maxval=-1 / np.pi ** 5))(inputs)
    #X = layers.Lambda(lambda x: tf.nn.gelu(x) * np.pi ** 3)(X)

    features = layers.Dropout(0.5)(X)
    features = X
    
    # Classify outputs.
    logits = layers.Dense(10)(features)
    # Create the Keras model.
    model = keras.Model(inputs=inputs, outputs=logits)
    
    return model

parasin_classifier = ParaSINNetwork()
parasin_classifier.summary()



batch_size = 1024
num_epochs = 10
weight_decay = 0.00001
learning_rate = 0.0001



def run_experiment(model):

    optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)

    model.compile(
        optimizer=optimizer,
        loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),
        metrics=[
            keras.metrics.SparseCategoricalAccuracy(name="accuracy"),
            keras.metrics.SparseTopKCategoricalAccuracy(2, name="top-2-accuracy"),
        ],
    )

    checkpoint_filepath = "/tmp/checkpoint"
    checkpoint_callback = keras.callbacks.ModelCheckpoint(
        checkpoint_filepath,
        monitor="val_accuracy",
        save_best_only=True,
        save_weights_only=True,
    )

    history = model.fit(
        x=x_train,
        y=y_train,
        batch_size=batch_size,
        epochs=num_epochs,
        validation_split=0.1,
        callbacks=[checkpoint_callback],
    )

    model.load_weights(checkpoint_filepath)
    _, accuracy, top_2_accuracy = model.evaluate(x_test, y_test)
    print(f"Test accuracy: {round(accuracy * 100, 2)}%")
    print(f"Test top 2 accuracy: {round(top_5_accuracy * 100, 2)}%")

    return history


#parasin_classifier = ParaSINNetwork()
history = run_experiment(parasin_classifier)




# plot Result
plt.plot(history.history['accuracy'])
plt.plot(history.history['val_accuracy'])
plt.legend(['train', 'validation'])
plt.xlabel("Epoch")
plt.ylabel("Accuracy")





















"""# CIFAR-10"""



num_classes = 10
input_shape = (32, 32, 3)

(x_train, y_train), (x_test, y_test) = keras.datasets.cifar10.load_data()

print(f"x_train shape: {x_train.shape} - y_train shape: {y_train.shape}")
print(f"x_test shape: {x_test.shape} - y_test shape: {y_test.shape}")

x_train = np.reshape(x_train, (50000,-1))
x_test = np.reshape(x_test, (10000,-1))

# Preprocess the data
x_train = x_train.astype('float32') / 127.5 - 1.0
x_test = x_test.astype('float32') / 127.5 - 1.0



def ParaSINNetwork():
    tf.keras.backend.clear_session()

    inputs = layers.Input(shape=(3 * 1024,))

    ## Parametric
    X = ParaSineLayer(3 * 1024, 256, is_first=True)(inputs)
    
    ## Gelu
    #X = layers.Dense(256, activation=None, use_bias=False, kernel_initializer=tf.keras.initializers.RandomUniform(minval=-1 / np.pi ** 5, maxval=-1 / np.pi ** 5))(inputs)
    #X = layers.Lambda(lambda x: tf.nn.gelu(x) * np.pi ** 3)(X)


   

    features = layers.Dropout(0.5)(X)
    #features = X
    
    # Classify outputs.
    logits = layers.Dense(10)(features)
    # Create the Keras model.
    model = keras.Model(inputs=inputs, outputs=logits)
    
    return model

parasin_classifier = ParaSINNetwork()
parasin_classifier.summary()

batch_size = 512
num_epochs = 10
weight_decay = 0.00001
learning_rate = 0.0001

def run_experiment(model):
    optimizer = tfa.optimizers.AdamW(
        learning_rate=learning_rate, weight_decay=weight_decay
    )

    model.compile(
        optimizer=optimizer,
        loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),
        metrics=[
            keras.metrics.SparseCategoricalAccuracy(name="accuracy"),
            keras.metrics.SparseTopKCategoricalAccuracy(2, name="top-2-accuracy"),
        ],
    )

    checkpoint_filepath = "/tmp/checkpoint"
    checkpoint_callback = keras.callbacks.ModelCheckpoint(
        checkpoint_filepath,
        monitor="val_accuracy",
        save_best_only=True,
        save_weights_only=True,
    )

    history = model.fit(
        x=x_train,
        y=y_train,
        batch_size=batch_size,
        epochs=num_epochs,
        validation_split=0.1,
        callbacks=[checkpoint_callback],
    )

    model.load_weights(checkpoint_filepath)
    _, accuracy, top_2_accuracy = model.evaluate(x_test, y_test)
    print(f"Test accuracy: {round(accuracy * 100, 2)}%")
    print(f"Test top 2 accuracy: {round(top_2_accuracy * 100, 2)}%")

    return history


#parasin_classifier = ParaSINNetwork()
history = run_experiment(parasin_classifier)





# plot Result
plt.plot(history.history['accuracy'])
plt.plot(history.history['val_accuracy'])
plt.legend(['train', 'validation'])
plt.xlabel("Epoch")
plt.ylabel("Accuracy")

