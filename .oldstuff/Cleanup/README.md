# ParamNeuRep

# New Architecture
ANN (single layer MLP) with infinitly many neurons is shown to approximate any continuous funciton. The new structure can capture n% of the function energy with m neurons!

## New Optimization
## New Training
Since the activation function and weights are training at the same time and the derivitive of the parametric activation function is highly non linear, New training schemes are required
For example
### For multi layer:
+ We can train one layer, then freez and add another layer and train
+ We can train weights and function parameters in separate iteration

#


# Ideas:

## Video
### Time coded with mlp to weights of the layers
